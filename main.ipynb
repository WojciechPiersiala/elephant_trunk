{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "registering..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wp/Studia/soft_robotics/elephant_trunk/trunk/lib/python3.12/site-packages/gymnasium/spaces/box.py:235: UserWarning: \u001b[33mWARN: Box low's precision lowered by casting to float32, current low.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/home/wp/Studia/soft_robotics/elephant_trunk/trunk/lib/python3.12/site-packages/gymnasium/spaces/box.py:305: UserWarning: \u001b[33mWARN: Box high's precision lowered by casting to float32, current high.dtype=float64\u001b[0m\n",
      "  gym.logger.warn(\n",
      "/home/wp/Studia/soft_robotics/elephant_trunk/trunk/lib/python3.12/site-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/wp/Studia/soft_robotics/elephant_trunk/trunk-agent folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "  0%|          | 1/1000 [00:01<30:54,  1.86s/it]"
     ]
    }
   ],
   "source": [
    "#!/home/wp/Studia/soft_robotics/elephant_trunk/trunk/bin/python\n",
    "#Enable Interactive Plots\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from manipulator.trunk_environment import TrunkEnv  # Import TrunkEnv\n",
    "\n",
    "exec(open(\"register.py\").read()) # register the project\n",
    "\n",
    "class TrunkAgent:\n",
    "    def __init__(self, env: gym.Env, learning_rate: float, epsilon: float, epsilon_decay: float, final_epsilon: float, discount_factor: float = 0.95, log: bool = False):\n",
    "        \"\"\"Initialize a reinforcement learning agent.\"\"\"\n",
    "        self.env = env\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.training_error = []\n",
    "        self.log = log  # Enable logging if True\n",
    "\n",
    "        # Discretize the observation space\n",
    "        N_BINS = 20\n",
    "        MAX_STEP = 0.2\n",
    "        self.obs_bins = [\n",
    "            np.linspace(-20, 20, N_BINS),  # x-effector\n",
    "            np.linspace(-30, 0, N_BINS),   # y-effector\n",
    "            np.linspace(-20, 20, N_BINS),  # x-target\n",
    "            np.linspace(-30, 0, N_BINS),   # y-target\n",
    "            np.linspace(-MAX_STEP, MAX_STEP, N_BINS),  # k1\n",
    "            np.linspace(-MAX_STEP, MAX_STEP, N_BINS),  # k2\n",
    "            np.linspace(-MAX_STEP, MAX_STEP, N_BINS),  # k3\n",
    "            np.linspace(-MAX_STEP, MAX_STEP, N_BINS)   # k4\n",
    "        ]\n",
    "\n",
    "        # Discretize the action space\n",
    "        ACTION_BINS = 20\n",
    "        self.action_bins = [\n",
    "            np.linspace(self.env.action_space.low[i], self.env.action_space.high[i], ACTION_BINS)\n",
    "            for i in range(self.env.action_space.shape[0])\n",
    "        ]\n",
    "\n",
    "        # Initialize Q-table with small random values\n",
    "        self.q_values = defaultdict(lambda: np.random.uniform(low=-0.01, high=0.01, size=(ACTION_BINS ** self.env.action_space.shape[0])))\n",
    "\n",
    "    def discretize_observation(self, obs):\n",
    "        \"\"\"Discretizes the continuous observation into bins.\"\"\"\n",
    "        discrete_obs = tuple(\n",
    "            np.clip(np.digitize(obs[i], self.obs_bins[i]) - 1, 0, len(self.obs_bins[i]) - 2)\n",
    "            for i in range(len(obs))\n",
    "        )\n",
    "        return discrete_obs\n",
    "\n",
    "    def discretize_action(self, action):\n",
    "        \"\"\"Discretizes a continuous action into bins.\"\"\"\n",
    "        discrete_action = tuple(\n",
    "            np.clip(np.digitize(action[i], self.action_bins[i]) - 1, 0, len(self.action_bins[i]) - 2)\n",
    "            for i in range(len(action))\n",
    "        )\n",
    "        return discrete_action\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        \"\"\"Returns the best action or a random one based on epsilon.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            if self.log:\n",
    "                print(\"Exploring: Random action chosen.\")\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            obs_tuple = self.discretize_observation(obs)\n",
    "            q_values = self.q_values[obs_tuple]\n",
    "            best_action_idx = np.random.choice(np.flatnonzero(q_values == q_values.max()))  # Tie-breaking\n",
    "            best_action = np.array([\n",
    "                self.action_bins[i][best_action_idx % len(self.action_bins[i])]\n",
    "                for i in range(len(self.action_bins))\n",
    "            ])\n",
    "            if self.log:\n",
    "                print(f\"Exploiting: Best action chosen - {best_action}\")\n",
    "            return best_action\n",
    "\n",
    "    def update(self, obs, action, reward, terminated, next_obs):\n",
    "        \"\"\"Updates a Q-value of an action.\"\"\"\n",
    "        tp_obs = self.discretize_observation(obs)\n",
    "        tp_action = self.discretize_action(action)\n",
    "        action_idx = np.ravel_multi_index(tp_action, [len(b) for b in self.action_bins])\n",
    "\n",
    "        tp_next_obs = self.discretize_observation(next_obs)\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[tp_next_obs])\n",
    "\n",
    "        # Normalize reward for stability\n",
    "        normalized_reward = np.clip(reward, -1.0, 1.0)\n",
    "\n",
    "        temporal_difference = normalized_reward + self.discount_factor * future_q_value - self.q_values[tp_obs][action_idx]\n",
    "        self.q_values[tp_obs][action_idx] += self.lr * temporal_difference\n",
    "        self.training_error.append(float(temporal_difference))\n",
    "\n",
    "        if self.log:\n",
    "            print(f\"Update: TD Error = {temporal_difference}\")\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        \"\"\"Decay epsilon using an exponential schedule.\"\"\"\n",
    "        self.epsilon = max(self.epsilon * self.epsilon_decay, self.final_epsilon)\n",
    "        if self.log:\n",
    "            print(f\"Epsilon decayed to {self.epsilon}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plotObs = True\n",
    "# Hyperparameters\n",
    "learning_rate = 0.05\n",
    "n_episodes = 100\n",
    "start_epsilon = 1.0\n",
    "final_epsilon = 0.05\n",
    "epsilon_decay = start_epsilon / (n_episodes)\n",
    "\n",
    "# Path to the folder\n",
    "folder_path = \"./trunk-agent\"\n",
    "\n",
    "# Clear the folder if it exists\n",
    "if os.path.exists(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)  # Remove files\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)  # Remove directories\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}: {e}\")\n",
    "else:\n",
    "    os.makedirs(folder_path)  # Create folder if it doesn't exist\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make(\"TrunkManipulator-v0\", render_mode=\"rgb_array\", max_steps=51, target=[4,-30])\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=\"trunk-agent\", name_prefix=\"eval\", episode_trigger=lambda x: x == n_episodes or x == 0)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env=env,buffer_length=n_episodes)\n",
    "agent = TrunkAgent(env=env, learning_rate=learning_rate, epsilon=start_epsilon, epsilon_decay=epsilon_decay, final_epsilon=final_epsilon)\n",
    "\n",
    "# Training loop\n",
    "episode_td_errors = []\n",
    "actions = []\n",
    "observations = []\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    episode_td_error = []\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        # print(f\"action : {action}\")\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.update(action=action, obs=obs, next_obs=next_obs, reward=reward, terminated=terminated)\n",
    "        obs = next_obs\n",
    "        done = terminated or truncated\n",
    "        episode_td_error.append(agent.training_error[-1])\n",
    "\n",
    "        actions.append(action) # debug\n",
    "        observations.append(obs) # debug\n",
    "\n",
    "        # print(f\"state: {obs}\")\n",
    "        # if episode % 100 == 0:\n",
    "            # print(f\"episode : {episode}, action : {action}, observation: {obs} \\n\")\n",
    "    episode_td_errors.append(np.mean(episode_td_error))\n",
    "    agent.decay_epsilon()\n",
    "\n",
    "\n",
    "# Plotting the training error\n",
    "plt.close('all')\n",
    "rolling_mean = np.convolve(episode_td_errors, np.ones(1) / 1, mode='valid')\n",
    "fig0, ax = plt.subplots(3,1,figsize=(10, 12))\n",
    "ax[0].plot(rolling_mean)\n",
    "ax[0].set_title(\"Training Error\")\n",
    "ax[0].set_xlabel(\"Episode\")\n",
    "ax[0].set_ylabel(\"Mean Temporal Difference\")\n",
    "\n",
    "ax[1].plot(env.return_queue)\n",
    "ax[1].set_title(\"Episode Rewards\")\n",
    "ax[1].set_xlabel(\"Episode\")\n",
    "ax[1].set_ylabel(\"Reward\")\n",
    "\n",
    "ax[2].plot(env.length_queue)\n",
    "ax[2].set_title(\"Episode Lengths\")\n",
    "ax[2].set_xlabel(\"Episode\")\n",
    "ax[2].set_ylabel(\"Length\")\n",
    "\n",
    "if n_episodes < 101 or plotObs:\n",
    "    fig1, ax1 = plt.subplots(2,1,figsize=(10, 4))\n",
    "    observations_array = np.array(observations)\n",
    "    actions_array = np.array(actions)\n",
    "    ax1[0].plot(actions_array[:,0], '-b')\n",
    "    ax1[0].plot(actions_array[:,1], '-r')\n",
    "    ax1[0].plot(actions_array[:,2], '-g')\n",
    "    ax1[0].plot(actions_array[:,3], '-m')\n",
    "    ax1[0].set_title(\"Actions - Curvatures updates\")\n",
    "    ax1[0].set_xlabel(\"Episode\")\n",
    "    ax1[0].set_ylabel(\"dK\")\n",
    "\n",
    "    ax1[1].plot(observations_array[:,4], '-b')\n",
    "    ax1[1].plot(observations_array[:,5], '-r')\n",
    "    ax1[1].plot(observations_array[:,6], '-g')\n",
    "    ax1[1].plot(observations_array[:,7], '-m')\n",
    "    ax1[1].set_title(\"Curvatures in each segment\")\n",
    "    ax1[1].set_xlabel(\"Episode\")\n",
    "    ax1[1].set_ylabel(\"K\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward over 10 evaluation episodes: -1998.8395943064327\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the agent\n",
    "total_rewards = []\n",
    "for _ in range(10):  # Evaluate for 100 episodes\n",
    "    obs, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        done = terminated or truncated\n",
    "    total_rewards.append(episode_reward)\n",
    "plt.close('all')\n",
    "print(f\"Average reward over 10 evaluation episodes: {np.mean(total_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div style=\"display: flex; justify-content: space-around; align-items: center;\">\n",
       "    <video width=\"640\" height=\"480\" controls autoplay loop muted>\n",
       "      <source src=\"trunk-agent/eval-episode-0.mp4\" type=\"video/mp4\">\n",
       "      Your browser does not support the video tag.\n",
       "    </video>\n",
       "    <p>First Episode</p>\n",
       "    <video width=\"640\" height=\"480\" controls autoplay loop muted>\n",
       "      <source src=\"trunk-agent/eval-episode-300.mp4\" type=\"video/mp4\">\n",
       "      Your browser does not support the video tag.\n",
       "    </video>\n",
       "    <p>Last episode</p>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "obs, info = env.reset()\n",
    "episode_over = False\n",
    "while not episode_over:\n",
    "\n",
    "  action = env.action_space.sample()  # replace with actual agent\n",
    "  obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "  episode_over = terminated or truncated\n",
    "plt.close('all')\n",
    "\n",
    "video_path1 = 'trunk-agent/eval-episode-' + str(n_episodes) + '.mp4'\n",
    "video_path2 = 'trunk-agent/eval-episode-' + str(0) + '.mp4'\n",
    "# Embed both videos in a single HTML block\n",
    "HTML(f\"\"\"\n",
    "<div style=\"display: flex; justify-content: space-around; align-items: center;\">\n",
    "    <video width=\"640\" height=\"480\" controls autoplay loop muted>\n",
    "      <source src=\"{video_path2}\" type=\"video/mp4\">\n",
    "      Your browser does not support the video tag.\n",
    "    </video>\n",
    "    <p>First Episode</p>\n",
    "    <video width=\"640\" height=\"480\" controls autoplay loop muted>\n",
    "      <source src=\"{video_path1}\" type=\"video/mp4\">\n",
    "      Your browser does not support the video tag.\n",
    "    </video>\n",
    "    <p>Last episode</p>\n",
    "</div>\n",
    "\"\"\")\n",
    "\n",
    "# Close the environment\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trunk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
