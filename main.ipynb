{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/home/wp/Studia/soft_robotics/gym/bin/python\n",
    "# Enable Interactive Plots\n",
    "%matplotlib widget\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "from manipulator.trunk_environment import TrunkEnv  # Import TrunkEnv\n",
    "\n",
    "class TrunkAgent:\n",
    "    def __init__(self, env: gym.Env, learning_rate: float, epsilon: float, epsilon_decay: float, final_epsilon: float, discount_factor: float = 0.95):\n",
    "        \"\"\"Initialize a reinforcement learning agent.\"\"\"\n",
    "        self.env = env\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.training_error = []\n",
    "        self.q_values = defaultdict(lambda: np.zeros(self.env.action_space.shape[0]))\n",
    "\n",
    "        # Define bins for discretization\n",
    "        N_BINS = 20\n",
    "        self.bins = [\n",
    "            np.linspace(-20, 20, N_BINS),  # x-effector\n",
    "            np.linspace(-30, 0, N_BINS),  # y-effector\n",
    "            np.linspace(-20, 20, N_BINS),  # x-target\n",
    "            np.linspace(-30, 0, N_BINS),  # y-target\n",
    "        ]\n",
    "\n",
    "    def discretize_observation(self, obs):\n",
    "        \"\"\"Discretizes the continuous observation into bins.\"\"\"\n",
    "        discrete_obs = tuple(np.digitize(obs[i], self.bins[i]) - 1 for i in range(len(obs)))\n",
    "        return discrete_obs\n",
    "\n",
    "    def get_action(self, obs) -> np.ndarray:\n",
    "        \"\"\"Returns the best action.\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            obs_tuple = self.discretize_observation(obs)\n",
    "            return np.argmax(self.q_values[obs_tuple])\n",
    "\n",
    "    def update(self, obs, action, reward, terminated, next_obs):\n",
    "        \"\"\"Updates a Q-value of an action.\"\"\"\n",
    "        tp_obs = self.discretize_observation(obs)\n",
    "        tp_next_obs = self.discretize_observation(next_obs)\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[tp_next_obs])\n",
    "        temporal_difference = reward + self.discount_factor * future_q_value - self.q_values[tp_obs][action]\n",
    "        self.q_values[tp_obs][action] += self.lr * temporal_difference\n",
    "        self.training_error.append(float(temporal_difference))\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon - self.epsilon_decay, self.final_epsilon)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "n_episodes = 10\n",
    "start_epsilon = 1.0\n",
    "final_epsilon = 0.05\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)\n",
    "\n",
    "# Remove previous recordings\n",
    "folder_path = \"/home/wp/Studia/soft_robotics/trunk-agent\"\n",
    "if os.path.exists(folder_path):\n",
    "    shutil.rmtree(folder_path)\n",
    "os.makedirs(folder_path)\n",
    "\n",
    "# Environment setup\n",
    "env = TrunkEnv(render_mode=\"rgb\")\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=\"trunk-agent\", name_prefix=\"eval\", episode_trigger=lambda x: x == n_episodes or x == 1)\n",
    "env = gym.wrappers.RecordEpisodeStatistics(env=env)\n",
    "agent = TrunkAgent(env=env, learning_rate=learning_rate, epsilon=start_epsilon, epsilon_decay=epsilon_decay, final_epsilon=final_epsilon)\n",
    "\n",
    "episode_td_errors = []\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    episode_td_error = []\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.update(action=action, obs=obs, next_obs=next_obs, reward=reward, terminated=terminated)\n",
    "        obs = next_obs\n",
    "        done = terminated or truncated\n",
    "        episode_td_error.append(agent.training_error[-1])\n",
    "    episode_td_errors.append(np.mean(episode_td_error))\n",
    "    agent.decay_epsilon()\n",
    "\n",
    "# Compute rolling mean of TD errors\n",
    "rolling_mean = np.convolve(episode_td_errors, np.ones(1) / 1, mode='valid')\n",
    "\n",
    "# Plot the training error\n",
    "fig, ax = plt.subplots(3,1,figsize=(10, 12))\n",
    "ax[0].plot(rolling_mean)\n",
    "ax[0].set_title(\"Training Error\")\n",
    "ax[0].set_xlabel(\"Episode\")\n",
    "ax[0].set_ylabel(\"Mean Temporal Difference\")\n",
    "\n",
    "ax[1].plot(env.return_queue)\n",
    "ax[1].set_title(\"Episode Rewards\")\n",
    "ax[1].set_xlabel(\"Episode\")\n",
    "ax[1].set_ylabel(\"Reward\")\n",
    "\n",
    "ax[2].plot(env.length_queue)\n",
    "ax[2].set_title(\"Episode Lengths\")\n",
    "ax[2].set_xlabel(\"Episode\")\n",
    "ax[2].set_ylabel(\"Length\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the agent\n",
    "total_rewards = []\n",
    "for _ in range(100):  # Evaluate for 100 episodes\n",
    "    obs, info = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "        done = terminated or truncated\n",
    "    total_rewards.append(episode_reward)\n",
    "\n",
    "print(f\"Average reward over 100 evaluation episodes: {np.mean(total_rewards)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trunk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
